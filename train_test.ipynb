{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9770eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "from tqdm.contrib import tenumerate\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def binarize_labels(\n",
    "      dataset: Dataset\n",
    "    , labels_to_pos: List[Any]\n",
    "    , labels_to_neg: List[Any]\n",
    "    , pos_label: int = 1\n",
    "    , neg_label: int = 0\n",
    "    , sample_seed: int = 42\n",
    "    , shuffle_seed: int = 42\n",
    ") -> Dataset:\n",
    "  \n",
    "    assert 'label' in dataset.features\n",
    "    assert set(labels_to_pos).isdisjoint(labels_to_neg)\n",
    "    random.seed(sample_seed)\n",
    "\n",
    "    pos_label2indices: Dict[Any, List] = {}\n",
    "    neg_label2indices: Dict[Any, List] = {}\n",
    "    for index, label in tenumerate(dataset['label']):\n",
    "        if label in labels_to_pos:\n",
    "            pos_label2indices.setdefault(label, []) \\\n",
    "                             .append(index)\n",
    "        if label in labels_to_neg:\n",
    "            neg_label2indices.setdefault(label, []) \\\n",
    "                             .append(index)\n",
    " \n",
    "    pos_num = sum(len(indices) for indices in pos_label2indices.values())\n",
    "    neg_num = sum(len(indices) for indices in neg_label2indices.values())\n",
    "    sample_ratio = min(pos_num, neg_num) / max(pos_num, neg_num)\n",
    "\n",
    "    if pos_num < neg_num:\n",
    "        for label, indices in neg_label2indices.items():\n",
    "            sample_size = ceil(sample_ratio * len(indices))\n",
    "            neg_label2indices[label] = random.sample(indices, sample_size)\n",
    "    else:\n",
    "        for label, indices in pos_label2indices.items():\n",
    "            sample_size = ceil(sample_ratio * len(indices))\n",
    "            pos_label2indices[label] = random.sample(indices, sample_size)\n",
    "\n",
    "    def _map_labels_to_pos(batch):\n",
    "        batch['label'] = [pos_label for _ in range(len(batch['label']))]\n",
    "        return batch\n",
    "    \n",
    "    def _map_labels_to_neg(batch):\n",
    "        batch['label'] = [neg_label for _ in range(len(batch['label']))]\n",
    "        return batch\n",
    "\n",
    "    dataset_balanced_binarized = concatenate_datasets(\n",
    "              [dataset.select(indices)\n",
    "                      .map(_map_labels_to_pos, batched=True, num_proc=4) \n",
    "               for indices in pos_label2indices.values()] \n",
    "            + [dataset.select(indices)\n",
    "                      .map(_map_labels_to_neg, batched=True, num_proc=4) \n",
    "               for indices in neg_label2indices.values()]\n",
    "        )\n",
    "\n",
    "    return dataset_balanced_binarized.shuffle(seed=shuffle_seed)\n",
    "\n",
    "\n",
    "def tokenize_premises_and_hypotheses(\n",
    "      batch: Dict[str, List]\n",
    "    , tokenizer: PreTrainedTokenizer\n",
    "):\n",
    "    # assumes all labels in the batch are available in `label_to_id`\n",
    "\n",
    "    return tokenizer(\n",
    "          text=batch['premise']\n",
    "        , text_pair=batch['hypothesis']\n",
    "        , truncation=True\n",
    "        , max_length=tokenizer.model_max_length\n",
    "        , padding='max_length'                      # CHANGE\n",
    "        , return_attention_mask=True\n",
    "        , return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "label_list = [ 'not_entailment', 'entailment' ]\n",
    "label_to_id = { v: i for i, v in enumerate(label_list) }\n",
    "id_to_label = { v: k for k, v in label_to_id.items() }\n",
    "\n",
    "snli = load_dataset('stanfordnlp/snli', cache_dir='.datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a54472f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load without fine-tuning\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'roberta-large'\n",
    "MODEL_CACHE_DIR = '.model/'\n",
    "\n",
    "config_wo_ft = AutoConfig.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , num_labels=len(label_list)\n",
    "    , finetuning_task='text-classification'\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    ")\n",
    "\n",
    "tokenizer_wo_ft = AutoTokenizer.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    "    , use_fast_tokenizer=True\n",
    ")\n",
    "\n",
    "model_wo_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , config=config_wo_ft\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74a1808b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8f23a302a74784ac6554d45aa703b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snli_test_wo_ft = binarize_labels(snli['test'], labels_to_pos=[0], labels_to_neg=[1,2]) \\\n",
    "                  .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer_wo_ft), batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49060c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6669dd6dbd814acdb96715dce896d777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=0.5166666666666667, P=0.5166666666666667, R=1.0, F=0.6813186813186813\n",
      "A=0.5114583333333333, P=0.5114583333333333, R=1.0, F=0.6767746381805652\n",
      "A=0.503125, P=0.503125, R=1.0, F=0.6694386694386695\n",
      "A=0.5015625, P=0.5015625, R=1.0, F=0.668054110301769\n",
      "A=0.49854166666666666, P=0.49854166666666666, R=1.0, F=0.6653691088558321\n",
      "A=0.4982638888888889, P=0.4982638888888889, R=1.0, F=0.6651216685979142\n",
      "A=0.5001488095238096, P=0.5001488095238096, R=1.0, F=0.6667989286777105\n",
      "A=0.49992578298946116, P=0.49992578298946116, R=1.0, F=0.6666006927263731\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.contrib import tenumerate\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_wo_ft.to(device)\n",
    "model_wo_ft.eval()\n",
    "\n",
    "truth = []\n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in tenumerate(snli_test_wo_ft.batch(batch_size=32)):\n",
    "        if i != 0 and i % 30 == 0:\n",
    "            p, r, f, _ = precision_recall_fscore_support(y_true=truth, y_pred=pred, pos_label=1, average='binary', zero_division=0)\n",
    "            a = accuracy_score(y_true=truth, y_pred=pred)\n",
    "            print('A={}, P={}, R={}, F={}'.format(a, p, r, f))\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        truth += batch['label']\n",
    "        output = model_wo_ft(input_ids, attention_mask, None)\n",
    "        pred += torch.argmax(output.logits, dim=1).tolist()\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_true=truth, y_pred=pred, pos_label=1, average='binary', zero_division=0)\n",
    "a = accuracy_score(y_true=truth, y_pred=pred)\n",
    "print('A={}, P={}, R={}, F={}'.format(a, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "927a42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load with fine-tuning\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = '.checkpoints/save/'\n",
    "MODEL_CACHE_DIR = '.model/'\n",
    "\n",
    "config_w_ft = AutoConfig.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , num_labels=len(label_list)\n",
    "    , finetuning_task='text-classification'\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    ")\n",
    "\n",
    "tokenizer_w_ft = AutoTokenizer.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    "    , use_fast_tokenizer=True\n",
    ")\n",
    "\n",
    "model_w_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , config=config_wo_ft\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8383127a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83f7acfde1e432b8e9d5100b7039a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd792dd00e664874831e49fdcb10e4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/6737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snli_test_w_ft = binarize_labels(snli['test'], labels_to_pos=[0], labels_to_neg=[1,2]) \\\n",
    "                 .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer_w_ft), batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "def65d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60328b6f253e48088d2b4ae399d1b2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batching examples:   0%|          | 0/6737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba08fae719914bf0a2c5d1aa5822c23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=0.9260416666666667, P=0.9363449691991786, R=0.9193548387096774, F=0.9277721261444557\n",
      "A=0.9317708333333333, P=0.9364102564102564, R=0.929735234215886, F=0.9330608073582013\n",
      "A=0.9350694444444444, P=0.9369806094182825, R=0.9337474120082816, F=0.935361216730038\n",
      "A=0.9338541666666667, P=0.9322647362978284, R=0.9361370716510904, F=0.9341968911917099\n",
      "A=0.934375, P=0.9321963394342762, R=0.9364814040952779, F=0.9343339587242027\n",
      "A=0.9340277777777778, P=0.9304979253112033, R=0.937630662020906, F=0.9340506768483166\n",
      "A=0.934970238095238, P=0.931777909037212, R=0.9387087176435585, F=0.9352304728027271\n",
      "A=0.9351343327890752, P=0.9319186560565871, R=0.9388361045130641, F=0.9353645910368289\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.contrib import tenumerate\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_w_ft.to(device)\n",
    "model_w_ft.eval()\n",
    "\n",
    "truth = []\n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in tenumerate(snli_test_w_ft.batch(batch_size=32)):\n",
    "        if i != 0 and i % 30 == 0:\n",
    "            p, r, f, _ = precision_recall_fscore_support(y_true=truth, y_pred=pred, pos_label=1, average='binary')\n",
    "            a = accuracy_score(y_true=truth, y_pred=pred)\n",
    "            print('A={}, P={}, R={}, F={}'.format(a, p, r, f))\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        truth += batch['label']\n",
    "        output = model_w_ft(input_ids, attention_mask, None)\n",
    "        pred += torch.argmax(output.logits, dim=1).tolist()\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_true=truth, y_pred=pred, pos_label=1, average='binary')\n",
    "a = accuracy_score(y_true=truth, y_pred=pred)\n",
    "print('A={}, P={}, R={}, F={}'.format(a, p, r, f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
