{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "from tqdm.contrib import tenumerate\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def binarize_labels(\n",
    "      dataset: Dataset\n",
    "    , labels_to_pos: List[Any]\n",
    "    , labels_to_neg: List[Any]\n",
    "    , pos_label: int = 1\n",
    "    , neg_label: int = 0\n",
    "    , sample_seed: int = 42\n",
    "    , shuffle_seed: int = 42\n",
    ") -> Dataset:\n",
    "  \n",
    "    assert 'label' in dataset.features\n",
    "    assert set(labels_to_pos).isdisjoint(labels_to_neg)\n",
    "    random.seed(sample_seed)\n",
    "\n",
    "    pos_label2indices: Dict[Any, List] = {}\n",
    "    neg_label2indices: Dict[Any, List] = {}\n",
    "    for index, label in tenumerate(dataset['label']):\n",
    "        if label in labels_to_pos:\n",
    "            pos_label2indices.setdefault(label, []) \\\n",
    "                             .append(index)\n",
    "        if label in labels_to_neg:\n",
    "            neg_label2indices.setdefault(label, []) \\\n",
    "                             .append(index)\n",
    " \n",
    "    pos_num = sum(len(indices) for indices in pos_label2indices.values())\n",
    "    neg_num = sum(len(indices) for indices in neg_label2indices.values())\n",
    "    sample_ratio = min(pos_num, neg_num) / max(pos_num, neg_num)\n",
    "\n",
    "    if pos_num < neg_num:\n",
    "        for label, indices in neg_label2indices.items():\n",
    "            sample_size = ceil(sample_ratio * len(indices))\n",
    "            neg_label2indices[label] = random.sample(indices, sample_size)\n",
    "    else:\n",
    "        for label, indices in pos_label2indices.items():\n",
    "            sample_size = ceil(sample_ratio * len(indices))\n",
    "            pos_label2indices[label] = random.sample(indices, sample_size)\n",
    "\n",
    "    def _map_labels_to_pos(batch):\n",
    "        batch['label'] = [pos_label for _ in range(len(batch['label']))]\n",
    "        return batch\n",
    "    \n",
    "    def _map_labels_to_neg(batch):\n",
    "        batch['label'] = [neg_label for _ in range(len(batch['label']))]\n",
    "        return batch\n",
    "\n",
    "    dataset_balanced_binarized = concatenate_datasets(\n",
    "              [dataset.select(indices)\n",
    "                      .map(_map_labels_to_pos, batched=True, num_proc=4) \n",
    "               for indices in pos_label2indices.values()] \n",
    "            + [dataset.select(indices)\n",
    "                      .map(_map_labels_to_neg, batched=True, num_proc=4) \n",
    "               for indices in neg_label2indices.values()]\n",
    "        )\n",
    "\n",
    "    return dataset_balanced_binarized.shuffle(seed=shuffle_seed)\n",
    "\n",
    "\n",
    "def tokenize_premises_and_hypotheses(\n",
    "      batch: Dict[str, List]\n",
    "    , tokenizer: PreTrainedTokenizer\n",
    "):\n",
    "    # assumes all labels in the batch are available in `label_to_id`\n",
    "\n",
    "    return tokenizer(\n",
    "          text=batch['premise']\n",
    "        , text_pair=batch['hypothesis']\n",
    "        , truncation=True\n",
    "        , max_length=tokenizer.model_max_length\n",
    "        , padding='max_length'                      # CHANGE\n",
    "        , return_attention_mask=True\n",
    "        , return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "label_list = [ 'not_entailment', 'entailment' ]\n",
    "label_to_id = { v: i for i, v in enumerate(label_list) }\n",
    "id_to_label = { v: k for k, v in label_to_id.items() }\n",
    "\n",
    "snli = load_dataset('stanfordnlp/snli', cache_dir='.datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54472f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load without fine-tuning\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = 'roberta-large'\n",
    "MODEL_CACHE_DIR = '.model/'\n",
    "\n",
    "config_wo_ft = AutoConfig.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , num_labels=len(label_list)\n",
    "    , finetuning_task='text-classification'\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    ")\n",
    "\n",
    "tokenizer_wo_ft = AutoTokenizer.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    "    , use_fast_tokenizer=True\n",
    ")\n",
    "\n",
    "model_wo_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , config=config_wo_ft\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_test_wo_ft = binarize_labels(snli['test'], labels_to_pos=[0], labels_to_neg=[1,2]) \\\n",
    "                  .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer_wo_ft), batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49060c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.contrib import tenumerate\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_wo_ft.to(device)\n",
    "model_wo_ft.eval()\n",
    "\n",
    "truth = []\n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in tenumerate(snli_test_wo_ft.batch(batch_size=32)):\n",
    "        if i != 0 and i % 30 == 0:\n",
    "            p, r, f, _ = precision_recall_fscore_support(y_true=truth, y_pred=pred, pos_label=1, average='binary', zero_division=0)\n",
    "            a = accuracy_score(y_true=truth, y_pred=pred)\n",
    "            print('A={}, P={}, R={}, F={}'.format(a, p, r, f))\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        truth += batch['label']\n",
    "        output = model_wo_ft(input_ids, attention_mask, None)\n",
    "        pred += torch.argmax(output.logits, dim=1).tolist()\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_true=truth, y_pred=pred, pos_label=1, average='binary', zero_division=0)\n",
    "a = accuracy_score(y_true=truth, y_pred=pred)\n",
    "print('A={}, P={}, R={}, F={}'.format(a, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927a42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load with fine-tuning\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = '.checkpoints/'\n",
    "MODEL_CACHE_DIR = '.model/'\n",
    "\n",
    "config_w_ft = AutoConfig.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , num_labels=len(label_list)\n",
    "    , finetuning_task='text-classification'\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    ")\n",
    "\n",
    "tokenizer_w_ft = AutoTokenizer.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    "    , use_fast_tokenizer=True\n",
    ")\n",
    "\n",
    "model_w_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , config=config_wo_ft\n",
    "    , cache_dir=MODEL_CACHE_DIR\n",
    "    , revision='main'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def65d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.contrib import tenumerate\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_w_ft.to(device)\n",
    "model_w_ft.eval()\n",
    "\n",
    "truth = []\n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in tenumerate(snli_test_wo_ft.batch(batch_size=32)):\n",
    "        if i != 0 and i % 30 == 0:\n",
    "            p, r, f, _ = precision_recall_fscore_support(y_true=truth, y_pred=pred, pos_label=1, average='binary')\n",
    "            a = accuracy_score(y_true=truth, y_pred=pred)\n",
    "            print('A={}, P={}, R={}, F={}'.format(a, p, r, f))\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        truth += batch['label']\n",
    "        output = model_w_ft(input_ids, attention_mask, None)\n",
    "        pred += torch.argmax(output.logits, dim=1).tolist()\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_true=truth, y_pred=pred, pos_label=1, average='binary')\n",
    "a = accuracy_score(y_true=truth, y_pred=pred)\n",
    "print('A={}, P={}, R={}, F={}'.format(a, p, r, f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
