{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c615d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pretrained RoBERTa for sequence classification, NLI\n",
    "# SNLI, MNLI, ANLI datasets for training\n",
    "# code ref: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76931a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = 'roberta-large'\n",
    "DATASET_CACHE_DIR = '.datasets/'\n",
    "TRAINER_OUTPUR_DIR = '.checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b157ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "snli = load_dataset('stanfordnlp/snli', cache_dir=DATASET_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from math import ceil\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from tqdm.contrib import tenumerate\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def binarize_labels(\n",
    "      dataset: Dataset\n",
    "    , labels_to_pos: List[Any]\n",
    "    , labels_to_neg: List[Any]\n",
    "    , pos_label: int = 1\n",
    "    , neg_label: int = 0\n",
    "    , sample_seed: int = 42\n",
    "    , shuffle_seed: int = 42\n",
    ") -> Dataset:\n",
    "  \n",
    "    assert 'label' in dataset.features\n",
    "    assert set(labels_to_pos).isdisjoint(labels_to_neg)\n",
    "    random.seed(sample_seed)\n",
    "\n",
    "    pos_label2indices: Dict[Any, List] = {}\n",
    "    neg_label2indices: Dict[Any, List] = {}\n",
    "    for index, label in tenumerate(dataset['label']):\n",
    "        if label in labels_to_pos:\n",
    "            pos_label2indices.setdefault(label, []) \\\n",
    "                             .append(index)\n",
    "        if label in labels_to_neg:\n",
    "            neg_label2indices.setdefault(label, []) \\\n",
    "                             .append(index)\n",
    " \n",
    "    pos_num = sum(len(indices) for indices in pos_label2indices.values())\n",
    "    neg_num = sum(len(indices) for indices in neg_label2indices.values())\n",
    "    sample_ratio = min(pos_num, neg_num) / max(pos_num, neg_num)\n",
    "\n",
    "    if pos_num < neg_num:\n",
    "        for label, indices in neg_label2indices.items():\n",
    "            sample_size = ceil(sample_ratio * len(indices))\n",
    "            neg_label2indices[label] = random.sample(indices, sample_size)\n",
    "    else:\n",
    "        for label, indices in pos_label2indices.items():\n",
    "            sample_size = ceil(sample_ratio * len(indices))\n",
    "            pos_label2indices[label] = random.sample(indices, sample_size)\n",
    "\n",
    "    def _map_labels_to_pos(batch):\n",
    "        batch['label'] = [pos_label for _ in range(len(batch['label']))]\n",
    "        return batch\n",
    "    \n",
    "    def _map_labels_to_neg(batch):\n",
    "        batch['label'] = [neg_label for _ in range(len(batch['label']))]\n",
    "        return batch\n",
    "\n",
    "    dataset_balanced_binarized = concatenate_datasets(\n",
    "              [dataset.select(indices)\n",
    "                      .map(_map_labels_to_pos, batched=True, num_proc=4) \n",
    "               for indices in pos_label2indices.values()] \n",
    "            + [dataset.select(indices)\n",
    "                      .map(_map_labels_to_neg, batched=True, num_proc=4) \n",
    "               for indices in neg_label2indices.values()]\n",
    "        )\n",
    "\n",
    "    return dataset_balanced_binarized.shuffle(seed=shuffle_seed)\n",
    "\n",
    "\n",
    "def tokenize_premises_and_hypotheses(\n",
    "      batch: Dict[str, List]\n",
    "    , tokenizer: PreTrainedTokenizer\n",
    "):\n",
    "    # assumes all labels in the batch are available in `label_to_id`\n",
    "\n",
    "    tokenized_batch = tokenizer(\n",
    "          batch['premise']\n",
    "        , batch['hypothesis']\n",
    "        , truncation=True\n",
    "        , max_length=tokenizer.model_max_length\n",
    "        , padding='max_length'\n",
    "        , return_attention_mask=True\n",
    "        , return_token_type_ids=True\n",
    "    )\n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a27d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "\n",
    "# make sure to `entailment` is the SECOND for positive class\n",
    "label_list = [ 'not_entailment', 'entailment' ]\n",
    "label_to_id = { v: i for i, v in enumerate(label_list) }\n",
    "id_to_label = { v: k for k, v in label_to_id.items() }\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , num_labels=len(label_list)\n",
    "    , finetuning_task='text-classification'\n",
    "    , problem_type='single_label_classification'\n",
    ")\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , config=config\n",
    ")\n",
    "model.config.label2id = label_to_id\n",
    "model.config.id2label = id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaaf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    ")\n",
    "\n",
    "snli_labels_to_pos = [0]     # `entailment` \n",
    "snli_lables_to_neg = [1, 2]  # `neutral`, `contradiction` \n",
    "\n",
    "snli_train = binarize_labels(\n",
    "              snli['train']\n",
    "            , labels_to_pos=snli_labels_to_pos\n",
    "            , labels_to_neg=snli_lables_to_neg\n",
    "            , pos_label=label_to_id['entailment']\n",
    "            , neg_label=label_to_id['not_entailment']\n",
    "      ) \\\n",
    "      .map(\n",
    "              lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer)\n",
    "            , batched=True\n",
    "            , num_proc=4\n",
    "      )\n",
    "\n",
    "snli_eval = binarize_labels(\n",
    "              snli['validation']\n",
    "            , labels_to_pos=snli_labels_to_pos\n",
    "            , labels_to_neg=snli_lables_to_neg\n",
    "            , pos_label=label_to_id['entailment']\n",
    "            , neg_label=label_to_id['not_entailment']\n",
    "      ) \\\n",
    "      .map(\n",
    "              lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer)\n",
    "            , batched=True\n",
    "            , num_proc=4\n",
    "      )\n",
    "\n",
    "snli_test = binarize_labels(\n",
    "              snli['test']\n",
    "            , labels_to_pos=snli_labels_to_pos\n",
    "            , labels_to_neg=snli_lables_to_neg\n",
    "            , pos_label=label_to_id['entailment']\n",
    "            , neg_label=label_to_id['not_entailment']\n",
    "      ) \\\n",
    "      .map(\n",
    "              lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer)\n",
    "            , batched=True\n",
    "            , num_proc=4\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc22759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset balance\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(snli_train['label']))\n",
    "print(Counter(snli_eval['label']))\n",
    "print(Counter(snli_test['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy\n",
    "import torch\n",
    "from transformers import EvalPrediction, Trainer, TrainingArguments\n",
    "from transformers.data import default_data_collator\n",
    "\n",
    "\n",
    "use_mixed_precision = True and torch.cuda.is_available()\n",
    "print(f'Using mixed precision: {use_mixed_precision}')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "          output_dir=TRAINER_OUTPUR_DIR\n",
    "        , overwrite_output_dir=True         # to overwrite the output directory\n",
    "        , eval_strategy='epoch'             # to evaluate every epoch\n",
    "        , save_strategy='epoch'             # to save the model every epoch\n",
    "        , learning_rate=5e-5\n",
    "        , num_train_epochs=3.0 \n",
    "        , per_device_train_batch_size=8\n",
    "        , gradient_accumulation_steps=1     # batch_size ~ this * per_device_train_epoch_batch_size\n",
    "        , per_device_eval_batch_size=16\n",
    "        , fp16=use_mixed_precision          # to use mixed precision training\n",
    "    )\n",
    "\n",
    "metrics = evaluate.combine([\n",
    "          evaluate.load('accuracy')\n",
    "        , evaluate.load('precision')\n",
    "        , evaluate.load('recall')\n",
    "        , evaluate.load('f1')\n",
    "    ])\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else \\\n",
    "            p.predictions\n",
    "    preds = numpy.argmax(preds, axis=1)\n",
    "    result = metrics.compute(predictions=preds, references=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d54988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "\n",
    "last_checkpoint = None\n",
    "if isdir(training_args.output_dir):\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "                'Output directory ({}) already exists and is not empty. ' \\\n",
    "                'Use --overwrite_output_dir to overcome.'\n",
    "                .format(training_args.output_dir)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd80e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_trainer = Trainer(\n",
    "          model=model\n",
    "        , args=training_args\n",
    "        , train_dataset=snli_train\n",
    "        , eval_dataset=snli_eval\n",
    "        , compute_metrics=compute_metrics\n",
    "        , processing_class=tokenizer\n",
    "        , data_collator=default_data_collator\n",
    "    )\n",
    "\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    snli_trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # HACK: when you interrrpt the training, GPU may not be initialized properly\n",
    "    del model\n",
    "    del snli_trainer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    raise KeyboardInterrupt('Training interrupted by user.')\n",
    "\n",
    "snli_trainer.save_model(output_dir='trained/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
