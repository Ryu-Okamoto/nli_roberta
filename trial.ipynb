{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c615d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pretrained RoBERTa for sequence classification, NLI\n",
    "# SNLI, MNLI, ANLI datasets for training\n",
    "# code ref: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31b157ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "snli = load_dataset('stanfordnlp/snli', cache_dir='.datasets/')\n",
    "mnli = load_dataset('nyu-mll/multi_nli', cache_dir='.datasets/')\n",
    "anli = load_dataset('facebook/anli', cache_dir='.datasets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4889d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def tokenize_premises_and_hypotheses(\n",
    "      batch: Dict[str, List]\n",
    "    , tokenizer: PreTrainedTokenizer\n",
    "    , label_to_id: Dict[Any, int]\n",
    "):\n",
    "    # assumes all labels in the batch are available in `label_to_id`\n",
    "\n",
    "    tokenized_batch = tokenizer(\n",
    "          batch['premise']\n",
    "        , batch['hypothesis']\n",
    "        , truncation=True\n",
    "        , max_length=tokenizer.model_max_length\n",
    "        , padding='max_length'\n",
    "        , return_attention_mask=True\n",
    "        , return_token_type_ids=True\n",
    "    )\n",
    "    tokenized_batch['label'] = [label_to_id[label] for label in batch['label']]\n",
    "    return tokenized_batch\n",
    "\n",
    "def are_labels_available(\n",
    "      batch: Dict[str, List]\n",
    "    , label_to_id: Dict[Any, int]\n",
    "):\n",
    "    return [label_to_id.get(label, -1) != -1 for label in batch['label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea47683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "      pretrained_model_name_or_path='roberta-large'\n",
    ")\n",
    "\n",
    "# convert 3-way NLI labels into binary classification\n",
    "# original label ids:\n",
    "#   - SNLI: 0 = entailment, 1 = neutral, 2 = contradiction, see: https://huggingface.co/datasets/stanfordnlp/snli\n",
    "#   - MNLI: 0 = entailment, 1 = neutral, 2 = contradiction, see: https://huggingface.co/datasets/nyu-mll/multi_nli\n",
    "#   - ANLI: 0 = entailment, 1 = neutral, 2 = contradiction, see: https://huggingface.co/datasets/facebook/anli\n",
    "snli_label_to_id = { 0: 0, 1: 1, 2: 1 }\n",
    "mnli_label_to_id = { 0: 0, 1: 1, 2: 1 }\n",
    "anli_label_to_id = { 0: 0, 1: 1, 2: 1 }\n",
    "\n",
    "snli_tokenized = snli.filter(lambda batch: are_labels_available(batch, snli_label_to_id), batched=True) \\\n",
    "                     .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer, snli_label_to_id), batched=True)\n",
    "mnli_tokenized = mnli.filter(lambda batch: are_labels_available(batch, mnli_label_to_id), batched=True) \\\n",
    "                     .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer, mnli_label_to_id), batched=True)\n",
    "anli_tokenized = anli.filter(lambda batch: are_labels_available(batch, anli_label_to_id), batched=True) \\\n",
    "                     .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer, anli_label_to_id), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57a27d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "\n",
    "label_list = [ 'entailment', 'not_entailment' ]\n",
    "label_to_id = { v: i for i, v in enumerate(label_list) }\n",
    "id_to_label = { v: k for k, v in label_to_id.items() }\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "      pretrained_model_name_or_path='roberta-large'\n",
    "    , num_labels=len(label_list)\n",
    "    , finetuning_task='text-classification'\n",
    "    , problem_type='single_label_classification'\n",
    ")\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "      pretrained_model_name_or_path='roberta-large'\n",
    "    , config=config\n",
    ")\n",
    "model.config.label2id = label_to_id\n",
    "model.config.id2label = id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9160e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "snli_train = snli_tokenized['train']\n",
    "mnli_train = mnli_tokenized['train']\n",
    "anli_train = concatenate_datasets([\n",
    "      anli_tokenized['train_r1']\n",
    "    , anli_tokenized['train_r2']\n",
    "    , anli_tokenized['train_r3']\n",
    "])\n",
    "\n",
    "snli_eval = snli_tokenized['validation']\n",
    "mnli_eval = concatenate_datasets([\n",
    "      mnli_tokenized['validation_matched']\n",
    "    , mnli_tokenized['validation_mismatched']\n",
    "])\n",
    "anli_eval = concatenate_datasets([\n",
    "      anli_tokenized['dev_r1']\n",
    "    , anli_tokenized['dev_r2']\n",
    "    , anli_tokenized['dev_r3']\n",
    "])\n",
    "\n",
    "snli_test = snli_tokenized['test']\n",
    "anli_test = concatenate_datasets([\n",
    "      anli_tokenized['test_r1']\n",
    "    , anli_tokenized['test_r2']\n",
    "    , anli_tokenized['test_r3']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffdd9386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f3a992ab3b4e90b34234658857a503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f6a44497f7429490ab564d8c71dd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded3f01974004bfb971da3ccbfe4e4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070cf6dc12734d6cb05ecd807e13bf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "import evaluate\n",
    "\n",
    "args = TrainingArguments(\n",
    "      output_dir='./output'\n",
    "    , eval_strategy='epoch'\n",
    "    , num_train_epochs=5\n",
    ")\n",
    "\n",
    "metrics = evaluate.combine([\n",
    "      evaluate.load('accuracy')\n",
    "    , evaluate.load('precision')\n",
    "    , evaluate.load('recall')\n",
    "    , evaluate.load('f1')\n",
    "])\n",
    "\n",
    "# TODO ref: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_classification.py#L584 ~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
