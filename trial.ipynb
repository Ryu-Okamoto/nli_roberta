{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c615d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pretrained RoBERTa for sequence classification, NLI\n",
    "# SNLI, MNLI, ANLI datasets for training\n",
    "# code ref: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76931a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = 'roberta-large'\n",
    "DATASET_CACHE_DIR = '.datasets/'\n",
    "TRAINER_OUTPUR_DIR = '.checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31b157ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "snli = load_dataset('stanfordnlp/snli', cache_dir=DATASET_CACHE_DIR)\n",
    "mnli = load_dataset('nyu-mll/multi_nli', cache_dir=DATASET_CACHE_DIR)\n",
    "anli = load_dataset('facebook/anli', cache_dir=DATASET_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4889d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def tokenize_premises_and_hypotheses(\n",
    "      batch: Dict[str, List]\n",
    "    , tokenizer: PreTrainedTokenizer\n",
    "    , label_to_id: Dict[Any, int]\n",
    "):\n",
    "    # assumes all labels in the batch are available in `label_to_id`\n",
    "\n",
    "    tokenized_batch = tokenizer(\n",
    "          batch['premise']\n",
    "        , batch['hypothesis']\n",
    "        , truncation=True\n",
    "        , max_length=tokenizer.model_max_length\n",
    "        , padding='max_length'\n",
    "        , return_attention_mask=True\n",
    "        , return_token_type_ids=True\n",
    "    )\n",
    "    tokenized_batch['label'] = [label_to_id[label] for label in batch['label']]\n",
    "    return tokenized_batch\n",
    "\n",
    "def are_labels_available(\n",
    "      batch: Dict[str, List]\n",
    "    , label_to_id: Dict[Any, int]\n",
    "):\n",
    "    return [label_to_id.get(label, -1) != -1 for label in batch['label']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57a27d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/r-okamot/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"text-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/r-okamot/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "\n",
    "label_list = [ 'entailment', 'not_entailment' ]\n",
    "label_to_id = { v: i for i, v in enumerate(label_list) }\n",
    "id_to_label = { v: k for k, v in label_to_id.items() }\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , num_labels=len(label_list)\n",
    "    , finetuning_task='text-classification'\n",
    "    , problem_type='single_label_classification'\n",
    ")\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    "    , config=config\n",
    ")\n",
    "model.config.label2id = label_to_id\n",
    "model.config.id2label = id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "baaaf52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /Users/r-okamot/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json\n",
      "loading file merges.txt from cache at /Users/r-okamot/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/r-okamot/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at /Users/r-okamot/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /Users/r-okamot/.cache/huggingface/hub/models--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "      pretrained_model_name_or_path=PRETRAINED_MODEL_NAME\n",
    ")\n",
    "\n",
    "snli_label_to_id = { 0: label_to_id['entailment'], 1: label_to_id['not_entailment'], 2: label_to_id['not_entailment'] } \n",
    "mnli_label_to_id = { 0: label_to_id['entailment'], 1: label_to_id['not_entailment'], 2: label_to_id['not_entailment'] }\n",
    "anli_label_to_id = { 0: label_to_id['entailment'], 1: label_to_id['not_entailment'], 2: label_to_id['not_entailment'] }  \n",
    "\n",
    "snli_tokenized = snli.filter(lambda batch: are_labels_available(batch, snli_label_to_id), batched=True) \\\n",
    "                     .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer, snli_label_to_id), batched=True)\n",
    "mnli_tokenized = mnli.filter(lambda batch: are_labels_available(batch, mnli_label_to_id), batched=True) \\\n",
    "                     .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer, mnli_label_to_id), batched=True)\n",
    "anli_tokenized = anli.filter(lambda batch: are_labels_available(batch, anli_label_to_id), batched=True) \\\n",
    "                     .map(lambda batch: tokenize_premises_and_hypotheses(batch, tokenizer, anli_label_to_id), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9160e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "snli_train = snli_tokenized['train']\n",
    "mnli_train = mnli_tokenized['train']\n",
    "anli_train = concatenate_datasets([\n",
    "      anli_tokenized['train_r1']\n",
    "    , anli_tokenized['train_r2']\n",
    "    , anli_tokenized['train_r3']\n",
    "])\n",
    "\n",
    "snli_eval = snli_tokenized['validation']\n",
    "mnli_eval = concatenate_datasets([\n",
    "      mnli_tokenized['validation_matched']\n",
    "    , mnli_tokenized['validation_mismatched']\n",
    "])\n",
    "anli_eval = concatenate_datasets([\n",
    "      anli_tokenized['dev_r1']\n",
    "    , anli_tokenized['dev_r2']\n",
    "    , anli_tokenized['dev_r3']\n",
    "])\n",
    "\n",
    "snli_test = snli_tokenized['test']\n",
    "anli_test = concatenate_datasets([\n",
    "      anli_tokenized['test_r1']\n",
    "    , anli_tokenized['test_r2']\n",
    "    , anli_tokenized['test_r3']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffdd9386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import  EvalPrediction, TrainingArguments, Trainer\n",
    "from transformers.data import default_data_collator\n",
    "import evaluate\n",
    "import numpy\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "      output_dir=TRAINER_OUTPUR_DIR\n",
    "    , eval_strategy='epoch'\n",
    "    , learning_rate=5e-5  # default\n",
    "    , num_train_epochs=3.0  # default\n",
    "    , per_device_train_batch_size=2\n",
    "    , per_device_eval_batch_size=2\n",
    ")\n",
    "\n",
    "metrics = evaluate.combine([\n",
    "      evaluate.load('accuracy')\n",
    "    , evaluate.load('precision')\n",
    "    , evaluate.load('recall')\n",
    "    , evaluate.load('f1')\n",
    "])\n",
    "\n",
    "def compute_metrics(preds: EvalPrediction):\n",
    "    preds = preds.predictions[0] if isinstance(preds.predictions, tuple) else \\\n",
    "            preds.predictions\n",
    "    preds = numpy.argmax(preds, axis=1)\n",
    "    result = metrics.compute(predictions=preds, references=preds.label_ids)\n",
    "    if len(result) > 1:\n",
    "        result['combined_score'] = numpy.mean(list(result.values())).item()\n",
    "    return result\n",
    "\n",
    "snli_trainer = Trainer(\n",
    "      model=model\n",
    "    , args=training_args\n",
    "    , train_dataset=snli_train\n",
    "    , eval_dataset=snli_eval\n",
    "    , compute_metrics=compute_metrics\n",
    "    , processing_class=tokenizer\n",
    "    , data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d54988",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3916212365.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m'Output directory ({}) already exists and is not empty. '.format(training_args.output_dir)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "last_checkpoint = None\n",
    "if isdir(training_args.output_dir):\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            'Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.'.format(training_args.output_dir)\n",
    "        )\n",
    "\n",
    "snli_trainer.train(resume_from_checkpoint=last_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
